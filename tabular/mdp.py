#!/usr/bin/env python
import numpy as np
from pdb import set_trace


class MDP:
    def __init__(self, transitions, rewards, gamma, start=None, terminal=None, max_value=1000):
        self.transitions = transitions
        num_actions = np.shape(transitions)[1]
        self.rewards = rewards
        self.gamma = gamma
        self.states = range(np.shape(self.transitions)[0])
        self.actions = range(np.shape(self.transitions)[1])
        self.max_value = max_value
        self.start = start
        self.terminal = len(self.states) - 1
        self.check_valid_mdp()

    def check_valid_mdp(self):
        is_valid = True
        # Need to be of the form S,A,T(S,A)
        if not (len(np.shape(self.transitions)) == 3):
            is_valid = False
        # check that state space size is same in both dims
        if not (np.shape(self.transitions)[0] == np.shape(self.transitions)[2]):
            is_valid = False
        # check that probabilities are valid
        for s in range(np.shape(self.transitions)[0]):
            for a in range(np.shape(self.transitions)[1]):
                prob_sum = 0
                for sprime in range(np.shape(self.transitions)[2]):
                    prob = self.transitions[s][a][sprime]
                    if prob < 0 or prob > 1:
                        is_valid = False
                    prob_sum += prob
                np.testing.assert_almost_equal(1.0, prob_sum)
        if self.gamma < 0 or self.gamma > 1:
            is_valid = False
        assert (is_valid)

    '''
    Policy Iteration from Sutton and Barto
    assumes discount factor of 0.99
    Deterministic policy iteration
    '''

    def policy_iteration(self, policy=None):
        # initialization
        if policy is None:
            policy = self.get_random_policy()

        policy_stable = False
        count = 0
        while not policy_stable:
            # policy evaluation
            V = self.policy_evaluation(policy)
            count += 1
            diff_count = 0
            # policy improvement
            policy_stable = True
            for state in self.states:
                old_action = policy[state]
                action_vals = np.dot(self.transitions[state, :, :], self.rewards + self.gamma * V).tolist()
                policy[state] = action_vals.index(max(action_vals))
                if not old_action == policy[state]:
                    diff_count += 1
                    policy_stable = False
        return (policy, V)

    def get_random_policy(self):
        policy = np.zeros(len(self.states), dtype=np.uint32)
        for state in self.states:
            policy[state] = np.random.randint(0, len(self.actions))
        return policy

    '''
    policy - deterministic policy, maps state to action
    -Deterministic policy evaluation
    '''

    def policy_evaluation(self, policy, theta=0.0001):
        V = np.zeros(len(self.states))
        count = 0
        while True:
            delta = 0
            for state in self.states:
                value = V[state]
                V[state] = np.dot(self.transitions[state, policy[state], :], self.rewards + self.gamma * V)
                delta = max(delta, np.abs(value - V[state]))
                # If divergence and policy has value -inf, return value function early
                if V[state] > self.max_value:
                    return V
                if V[state] < -self.max_value:
                    return V
            if delta < theta:
                break
        return V

    def policy_q_evaluation(self, policy):
        V = self.policy_evaluation(policy)
        Q = np.zeros(np.shape(self.transitions)[0:2])
        for state in self.states:
            for action in self.actions:
                Q[state, action] = np.dot(self.transitions[state, action, :], self.rewards + self.gamma * V)
        return Q

    def value_iteration(self, theta=0.0001):
        V = np.zeros(len(self.states))
        while True:
            delta = 0
            for state in self.states:
                v = V[state]
                V[state] = np.amax(np.dot(self.transitions[state, :, :], self.rewards + self.gamma * V))
                delta = max(delta, np.abs(v - V[state]))
            if delta < theta:
                break
        return V

    def q_value_iteration(self, theta=0.0001):
        Q = np.zeros((len(self.states), len(self.actions)))
        while True:
            delta = 0
            for state in self.states:
                for action in self.actions:
                    q = Q[state, action]
                    Q[state, action] = np.dot(self.transitions[state, action, :],
                                              self.rewards + self.gamma * np.amax(Q, axis=1))
                    delta = max(delta, np.abs(q - Q[state, action]))
            if delta < theta:
                break
        return Q

    '''
    Takes a state and action, returns next state
    Chooses a next state according to transition 
    probabilities
    '''

    def act(self, state, action):
        next_state = np.random.choice(self.states, p=self.transitions[state, action, :])
        return self.rewards[next_state], next_state
